Reconstruction Metrics
=====================

Mean reconstruction error: nan
Initial spread: 3.000000
Final spread: nan

Training Metrics
================

Best training loss: 6.608246
Final training loss: 6.617650
Best epoch: 1618

Configuration
=============

Config: linear_arc
Drift: linear
Points: 5000
Epsilon: 0.15
T: 20.0
dt: 0.01


======================================================================
  SDE FORWARD + BACKWARD SIMULATION
  Configuration: LINEAR_ARC
  Mode: RANDOM TIME SAMPLING
======================================================================

Random seed: 42
­ЪЊЂ Output directory: outputs/linear_arc_20251016_101536

SDE Configuration:
  Рђб Dim: 2D
  Рђб T: 20.0 s
  Рђб dt: 0.01 s
  Рђб n_steps: 2000
  Рђб epsilon: 0.15

Initial Manifold:
  Рђб Type: ARC
  Рђб Points: 5000
  Рђб Radius: 3.0
  Рђб Arc fraction: 0.35

Drift: LINEAR

РєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњ
  FORWARD SDE (t: 0 Рєњ T)
РєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњРєњ
  Рђб Snapshots collected: 200
РюЊ Forward completed!
  Рђб Snapshots: 200
  Рђб Time range: [0.110, 19.738]
  Рђб Final positions saved: (5000, 2)
­ЪњЙ Forward plot saved to: outputs/linear_arc_20251016_101536/plots/forward_trajectory.png

­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа
  NEURAL NETWORK SCORE TRAINING
­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа­ЪДа

Device: cuda
РюЊ ScoreNet initialized (206850 parameters)

Training neural network...
  Рђб Epochs: 2000
  Рђб Batch size: 256
  Рђб Learning rate: 0.0005
  Рђб Denoising ¤Ѓ: 0.2

Training Configuration:
  Рђб Dataset: 1000000 points
  Рђб Epochs: 2000
  Рђб Batch size: 256
  Рђб Learning rate: 0.0005
  Рђб Denoising ¤Ѓ: 0.2
  Рђб Weight decay: 0.0001
  Рђб Scheduler: cosine
  Рђб Grad clip: 1.0
  Рђб Save best: True
  Рђб Device: cuda

Training:   0%|                                                                                           | 1/2000 [00:04<2:28:35,  4.46s/it, loss=7.571320, best=7.571320, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.848633
Training:   0%|                                                                                           | 2/2000 [00:08<2:26:23,  4.40s/it, loss=6.848633, best=6.848633, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.798268
Training:   0%|РќЈ                                                                                          | 3/2000 [00:13<2:27:28,  4.43s/it, loss=6.798268, best=6.798268, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.762133
Training:   0%|РќЈ                                                                                          | 4/2000 [00:17<2:26:31,  4.40s/it, loss=6.762133, best=6.762133, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.750672
Training:   0%|РќЈ                                                                                          | 5/2000 [00:22<2:26:16,  4.40s/it, loss=6.750672, best=6.750672, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.735491
Training:   0%|Рќј                                                                                          | 6/2000 [00:26<2:25:49,  4.39s/it, loss=6.735491, best=6.735491, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.734644
Training:   0%|Рќј                                                                                          | 8/2000 [00:35<2:24:51,  4.36s/it, loss=6.735157, best=6.734644, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.718193
Training:   0%|РќЇ                                                                                         | 10/2000 [00:43<2:24:07,  4.35s/it, loss=6.730831, best=6.718193, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.712217
Training:   1%|РќЇ                                                                                         | 11/2000 [00:48<2:24:01,  4.34s/it, loss=6.712217, best=6.712217, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.706902
Training:   1%|Рќї                                                                                         | 12/2000 [00:52<2:23:56,  4.34s/it, loss=6.706902, best=6.706902, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.699356
Training:   1%|РќІ                                                                                         | 15/2000 [01:05<2:23:11,  4.33s/it, loss=6.703212, best=6.699356, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.699226
Training:   1%|Рќі                                                                                         | 18/2000 [01:18<2:22:54,  4.33s/it, loss=6.700866, best=6.699226, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.690902
Training:   1%|Рќі                                                                                         | 19/2000 [01:22<2:22:52,  4.33s/it, loss=6.690902, best=6.690902, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.682370
Training:   1%|РќЅ                                                                                         | 21/2000 [01:31<2:23:32,  4.35s/it, loss=6.697511, best=6.682370, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.681002
Training:   1%|РќЅ                                                                                         | 22/2000 [01:35<2:23:32,  4.35s/it, loss=6.681002, best=6.681002, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.675270
Training:   2%|РќѕРќІ                                                                                        | 37/2000 [02:40<2:21:03,  4.31s/it, loss=6.679521, best=6.675270, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.669587
Training:   2%|РќѕРќІ                                                                                        | 38/2000 [02:44<2:20:45,  4.30s/it, loss=6.669587, best=6.669587, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.658837
Training:   2%|РќѕРќі                                                                                        | 39/2000 [02:49<2:21:11,  4.32s/it, loss=6.658837, best=6.658837, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.655608
Training:   3%|РќѕРќѕРќЇ                                                                                       | 54/2000 [03:53<2:19:07,  4.29s/it, loss=6.668694, best=6.655608, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.653502
Training:   4%|РќѕРќѕРќѕРќј                                                                                      | 74/2000 [05:20<2:18:42,  4.32s/it, loss=6.678298, best=6.653502, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.652899
Training:   4%|РќѕРќѕРќѕРќї                                                                                      | 79/2000 [05:41<2:18:23,  4.32s/it, loss=6.660312, best=6.652899, lr=5.00e-04]
­ЪњЙ Best model saved! Loss: 6.648749
Training:   5%|РќѕРќѕРќѕРќѕРќЇ                                                                                     | 99/2000 [07:12<2:15:48,  4.29s/it, loss=6.651404, best=6.648749, lr=4.99e-04]
Epoch 100/2000 - Loss: 6.651404, Best: 6.648749, LR: 4.99e-04
Training:   5%|РќѕРќѕРќѕРќѕРќі                                                                                    | 107/2000 [07:42<2:16:04,  4.31s/it, loss=6.662694, best=6.648749, lr=4.99e-04]
­ЪњЙ Best model saved! Loss: 6.645412
Training:   6%|РќѕРќѕРќѕРќѕРќѕРќї                                                                                   | 126/2000 [09:04<2:14:43,  4.31s/it, loss=6.645861, best=6.645412, lr=4.98e-04]
­ЪњЙ Best model saved! Loss: 6.643950
Training:   8%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќЇ                                                                                 | 168/2000 [12:05<2:11:28,  4.31s/it, loss=6.664028, best=6.643950, lr=4.96e-04]
­ЪњЙ Best model saved! Loss: 6.641922
Training:   9%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќі                                                                                 | 175/2000 [12:36<2:11:26,  4.32s/it, loss=6.661932, best=6.641922, lr=4.95e-04]
­ЪњЙ Best model saved! Loss: 6.638936
Training:  10%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќі                                                                                | 199/2000 [14:24<2:10:07,  4.34s/it, loss=6.654198, best=6.638936, lr=4.93e-04]
Epoch 200/2000 - Loss: 6.654198, Best: 6.638936, LR: 4.93e-04
Training:  13%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќј                                                                             | 255/2000 [18:20<2:05:19,  4.31s/it, loss=6.645435, best=6.638936, lr=4.87e-04]
­ЪњЙ Best model saved! Loss: 6.637999
Training:  15%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќј                                                                           | 299/2000 [21:34<2:02:03,  4.31s/it, loss=6.647630, best=6.637999, lr=4.81e-04]
Epoch 300/2000 - Loss: 6.647630, Best: 6.637999, LR: 4.81e-04
Training:  16%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕ                                                                           | 317/2000 [22:47<2:00:42,  4.30s/it, loss=6.649298, best=6.637999, lr=4.79e-04]
­ЪњЙ Best model saved! Loss: 6.637237
Training:  16%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќІ                                                                          | 330/2000 [23:43<1:59:33,  4.30s/it, loss=6.649123, best=6.637237, lr=4.76e-04]
­ЪњЙ Best model saved! Loss: 6.634734
Training:  19%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќЅ                                                                        | 382/2000 [27:27<1:55:57,  4.30s/it, loss=6.649038, best=6.634734, lr=4.67e-04]
­ЪњЙ Best model saved! Loss: 6.634363
Training:  20%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќї                                                                       | 394/2000 [28:19<1:55:13,  4.30s/it, loss=6.650773, best=6.634363, lr=4.65e-04]
­ЪњЙ Best model saved! Loss: 6.629349
Training:  20%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќі                                                                       | 399/2000 [28:45<1:55:13,  4.32s/it, loss=6.650066, best=6.629349, lr=4.64e-04]
Epoch 400/2000 - Loss: 6.650066, Best: 6.629349, LR: 4.64e-04
Training:  24%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќј                                                                   | 478/2000 [34:20<1:49:10,  4.30s/it, loss=6.644551, best=6.629349, lr=4.46e-04]
­ЪњЙ Best model saved! Loss: 6.629003
Training:  25%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќЈ                                                                  | 499/2000 [35:55<1:47:48,  4.31s/it, loss=6.647230, best=6.629003, lr=4.41e-04]
Epoch 500/2000 - Loss: 6.647230, Best: 6.629003, LR: 4.41e-04
Training:  30%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќј                                                              | 591/2000 [42:30<1:41:16,  4.31s/it, loss=6.642761, best=6.629003, lr=4.16e-04]
­ЪњЙ Best model saved! Loss: 6.625730
Training:  30%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќІ                                                              | 599/2000 [43:08<1:39:58,  4.28s/it, loss=6.655885, best=6.625730, lr=4.13e-04]
Epoch 600/2000 - Loss: 6.655885, Best: 6.625730, LR: 4.13e-04
Training:  35%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕ                                                          | 699/2000 [50:18<1:33:26,  4.31s/it, loss=6.639262, best=6.625730, lr=3.82e-04]
Epoch 700/2000 - Loss: 6.639262, Best: 6.625730, LR: 3.82e-04
Training:  40%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќї                                                     | 799/2000 [57:28<1:26:30,  4.32s/it, loss=6.639958, best=6.625730, lr=3.47e-04]
Epoch 800/2000 - Loss: 6.639958, Best: 6.625730, LR: 3.47e-04
Training:  42%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќј                                                  | 834/2000 [1:00:11<1:36:18,  4.96s/it, loss=6.636367, best=6.625730, lr=3.35e-04]
­ЪњЙ Best model saved! Loss: 6.621238
Training:  45%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕ                                                | 899/2000 [1:05:31<1:30:03,  4.91s/it, loss=6.648846, best=6.621238, lr=3.10e-04]
Epoch 900/2000 - Loss: 6.648846, Best: 6.621238, LR: 3.10e-04
Training:  49%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќЅ                                            | 988/2000 [1:12:45<1:24:38,  5.02s/it, loss=6.638624, best=6.621238, lr=2.77e-04]
­ЪњЙ Best model saved! Loss: 6.619956
Training:  50%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќЇ                                           | 999/2000 [1:13:45<1:22:52,  4.97s/it, loss=6.646456, best=6.619956, lr=2.72e-04]
Epoch 1000/2000 - Loss: 6.646456, Best: 6.619956, LR: 2.72e-04
Training:  51%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќЅ                                          | 1023/2000 [1:15:38<1:20:11,  4.93s/it, loss=6.629480, best=6.619956, lr=2.63e-04]
­ЪњЙ Best model saved! Loss: 6.617828
Training:  55%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќј                                      | 1099/2000 [1:21:38<1:09:23,  4.62s/it, loss=6.639067, best=6.617828, lr=2.33e-04]
Epoch 1100/2000 - Loss: 6.639067, Best: 6.617828, LR: 2.33e-04
Training:  60%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќі                                   | 1199/2000 [1:29:05<58:12,  4.36s/it, loss=6.634614, best=6.617828, lr=1.95e-04]
Epoch 1200/2000 - Loss: 6.634614, Best: 6.617828, LR: 1.95e-04
Training:  61%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќІ                                  | 1219/2000 [1:30:29<56:57,  4.38s/it, loss=6.635920, best=6.617828, lr=1.88e-04]
­ЪњЙ Best model saved! Loss: 6.614837
Training:  61%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќі                                 | 1228/2000 [1:31:09<1:00:37,  4.71s/it, loss=6.638646, best=6.614837, lr=1.84e-04]
­ЪњЙ Best model saved! Loss: 6.613796
Training:  65%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќЈ                              | 1299/2000 [1:36:47<54:55,  4.70s/it, loss=6.634629, best=6.613796, lr=1.58e-04]
Epoch 1300/2000 - Loss: 6.634629, Best: 6.613796, LR: 1.58e-04
Training:  68%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќі                            | 1360/2000 [1:41:33<50:57,  4.78s/it, loss=6.632567, best=6.613796, lr=1.37e-04]
­ЪњЙ Best model saved! Loss: 6.611748
Training:  70%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќї                          | 1399/2000 [1:44:32<44:31,  4.44s/it, loss=6.636470, best=6.611748, lr=1.23e-04]
Epoch 1400/2000 - Loss: 6.636470, Best: 6.611748, LR: 1.23e-04
Training:  75%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќЅ                      | 1499/2000 [1:52:06<36:09,  4.33s/it, loss=6.636359, best=6.611748, lr=9.18e-05]
Epoch 1500/2000 - Loss: 6.636359, Best: 6.611748, LR: 9.18e-05
Training:  80%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќј                 | 1599/2000 [1:59:20<28:57,  4.33s/it, loss=6.631925, best=6.611748, lr=6.43e-05]
Epoch 1600/2000 - Loss: 6.631925, Best: 6.611748, LR: 6.43e-05
Training:  80%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќЇ                 | 1602/2000 [1:59:29<28:55,  4.36s/it, loss=6.621115, best=6.611748, lr=6.38e-05]
­ЪњЙ Best model saved! Loss: 6.610153
Training:  81%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќЈ                | 1618/2000 [2:00:38<27:42,  4.35s/it, loss=6.635766, best=6.610153, lr=5.98e-05]
­ЪњЙ Best model saved! Loss: 6.608246
Training:  85%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќі             | 1699/2000 [2:06:36<21:50,  4.35s/it, loss=6.627280, best=6.608246, lr=4.15e-05]
Epoch 1700/2000 - Loss: 6.627280, Best: 6.608246, LR: 4.15e-05
Training:  90%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќЈ        | 1799/2000 [2:13:51<14:39,  4.38s/it, loss=6.628145, best=6.608246, lr=2.38e-05]
Epoch 1800/2000 - Loss: 6.628145, Best: 6.608246, LR: 2.38e-05
Training:  95%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќї    | 1899/2000 [2:21:07<07:12,  4.28s/it, loss=6.625599, best=6.608246, lr=1.18e-05]
Epoch 1900/2000 - Loss: 6.625599, Best: 6.608246, LR: 1.18e-05
Training: 100%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќЅ| 1999/2000 [2:28:24<00:04,  4.86s/it, loss=6.617650, best=6.608246, lr=5.76e-06]
Epoch 2000/2000 - Loss: 6.617650, Best: 6.608246, LR: 5.76e-06
Training: 100%|РќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕРќѕ| 2000/2000 [2:28:24<00:00,  4.45s/it, loss=6.617650, best=6.608246, lr=5.76e-06]

­ЪњЙ Final model saved to: outputs/linear_arc_20251016_101536/checkpoints/final_model.pt
­ЪњЙ Best model saved to: outputs/linear_arc_20251016_101536/checkpoints/best_model.pt
   Best loss: 6.608246

РюЊ Training completed! Final loss: 6.617650

­Ъћё Loading best model for backward SDE...
/home/barbara/Downloads/LARGE_DEVIATIONS_DM/Notebooks/my_tries/score_sde_pytorch/__personal_implementation__/ScoreNet.py:325: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(best_model_path, map_location=self.device)
РюЊ Best model loaded from outputs/linear_arc_20251016_101536/checkpoints/best_model.pt
  Epoch: 1618, Loss: 6.608246
­ЪњЙ Loss plot saved to: outputs/linear_arc_20251016_101536/plots/training_loss.png
РюЊ Model saved to models/linear_arc_trained.pt

РюЊ Model saved to: models/linear_arc_trained.pt

РюЊ Neural network score function ready!

РєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљ
  BACKWARD SDE (t: T Рєњ 0)
РєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљРєљ

Running backward SDE...
  Рђб Starting from 5000 points at t=19.74
РюЊ Backward completed!
  Рђб Snapshots: 2001
  Рђб Final time: 0.000 s

======================================================================
  RECONSTRUCTION RESULTS
======================================================================
  Рђб Mean reconstruction error: nan
  Рђб Initial spread: 3.0000
  Рђб Final spread: nan
­ЪњЙ Metrics saved to: outputs/linear_arc_20251016_101536/metrics.txt
­ЪњЙ Comparison plot saved to: outputs/linear_arc_20251016_101536/plots/forward_backward_comparison.png

======================================================================
РюЊ Simulation completed!
­ЪЊЂ All outputs saved to: outputs/linear_arc_20251016_101536
======================================================================

