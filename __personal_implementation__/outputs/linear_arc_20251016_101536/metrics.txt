Reconstruction Metrics
=====================

Mean reconstruction error: nan
Initial spread: 3.000000
Final spread: nan

Training Metrics
================

Best training loss: 6.608246
Final training loss: 6.617650
Best epoch: 1618

Configuration
=============

Config: linear_arc
Drift: linear
Points: 5000
Epsilon: 0.15
T: 20.0
dt: 0.01


======================================================================
  SDE FORWARD + BACKWARD SIMULATION
  Configuration: LINEAR_ARC
  Mode: RANDOM TIME SAMPLING
======================================================================

Random seed: 42
ğŸ“ Output directory: outputs/linear_arc_20251016_101536

SDE Configuration:
  â€¢ Dim: 2D
  â€¢ T: 20.0 s
  â€¢ dt: 0.01 s
  â€¢ n_steps: 2000
  â€¢ epsilon: 0.15

Initial Manifold:
  â€¢ Type: ARC
  â€¢ Points: 5000
  â€¢ Radius: 3.0
  â€¢ Arc fraction: 0.35

Drift: LINEAR

â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’
  FORWARD SDE (t: 0 â†’ T)
â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’
  â€¢ Snapshots collected: 200
âœ“ Forward completed!
  â€¢ Snapshots: 200
  â€¢ Time range: [0.110, 19.738]
  â€¢ Final positions saved: (5000, 2)
ğŸ’¾ Forward plot saved to: outputs/linear_arc_20251016_101536/plots/forward_trajectory.png

ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ 
  NEURAL NETWORK SCORE TRAINING
ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ 

Device: cuda
âœ“ ScoreNet initialized (206850 parameters)

Training neural network...
  â€¢ Epochs: 2000
  â€¢ Batch size: 256
  â€¢ Learning rate: 0.0005
  â€¢ Denoising Ïƒ: 0.2

Training Configuration:
  â€¢ Dataset: 1000000 points
  â€¢ Epochs: 2000
  â€¢ Batch size: 256
  â€¢ Learning rate: 0.0005
  â€¢ Denoising Ïƒ: 0.2
  â€¢ Weight decay: 0.0001
  â€¢ Scheduler: cosine
  â€¢ Grad clip: 1.0
  â€¢ Save best: True
  â€¢ Device: cuda

Training:   0%|                                                                                           | 1/2000 [00:04<2:28:35,  4.46s/it, loss=7.571320, best=7.571320, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.848633
Training:   0%|                                                                                           | 2/2000 [00:08<2:26:23,  4.40s/it, loss=6.848633, best=6.848633, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.798268
Training:   0%|â–                                                                                          | 3/2000 [00:13<2:27:28,  4.43s/it, loss=6.798268, best=6.798268, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.762133
Training:   0%|â–                                                                                          | 4/2000 [00:17<2:26:31,  4.40s/it, loss=6.762133, best=6.762133, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.750672
Training:   0%|â–                                                                                          | 5/2000 [00:22<2:26:16,  4.40s/it, loss=6.750672, best=6.750672, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.735491
Training:   0%|â–                                                                                          | 6/2000 [00:26<2:25:49,  4.39s/it, loss=6.735491, best=6.735491, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.734644
Training:   0%|â–                                                                                          | 8/2000 [00:35<2:24:51,  4.36s/it, loss=6.735157, best=6.734644, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.718193
Training:   0%|â–                                                                                         | 10/2000 [00:43<2:24:07,  4.35s/it, loss=6.730831, best=6.718193, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.712217
Training:   1%|â–                                                                                         | 11/2000 [00:48<2:24:01,  4.34s/it, loss=6.712217, best=6.712217, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.706902
Training:   1%|â–Œ                                                                                         | 12/2000 [00:52<2:23:56,  4.34s/it, loss=6.706902, best=6.706902, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.699356
Training:   1%|â–‹                                                                                         | 15/2000 [01:05<2:23:11,  4.33s/it, loss=6.703212, best=6.699356, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.699226
Training:   1%|â–Š                                                                                         | 18/2000 [01:18<2:22:54,  4.33s/it, loss=6.700866, best=6.699226, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.690902
Training:   1%|â–Š                                                                                         | 19/2000 [01:22<2:22:52,  4.33s/it, loss=6.690902, best=6.690902, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.682370
Training:   1%|â–‰                                                                                         | 21/2000 [01:31<2:23:32,  4.35s/it, loss=6.697511, best=6.682370, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.681002
Training:   1%|â–‰                                                                                         | 22/2000 [01:35<2:23:32,  4.35s/it, loss=6.681002, best=6.681002, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.675270
Training:   2%|â–ˆâ–‹                                                                                        | 37/2000 [02:40<2:21:03,  4.31s/it, loss=6.679521, best=6.675270, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.669587
Training:   2%|â–ˆâ–‹                                                                                        | 38/2000 [02:44<2:20:45,  4.30s/it, loss=6.669587, best=6.669587, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.658837
Training:   2%|â–ˆâ–Š                                                                                        | 39/2000 [02:49<2:21:11,  4.32s/it, loss=6.658837, best=6.658837, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.655608
Training:   3%|â–ˆâ–ˆâ–                                                                                       | 54/2000 [03:53<2:19:07,  4.29s/it, loss=6.668694, best=6.655608, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.653502
Training:   4%|â–ˆâ–ˆâ–ˆâ–                                                                                      | 74/2000 [05:20<2:18:42,  4.32s/it, loss=6.678298, best=6.653502, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.652899
Training:   4%|â–ˆâ–ˆâ–ˆâ–Œ                                                                                      | 79/2000 [05:41<2:18:23,  4.32s/it, loss=6.660312, best=6.652899, lr=5.00e-04]
ğŸ’¾ Best model saved! Loss: 6.648749
Training:   5%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                     | 99/2000 [07:12<2:15:48,  4.29s/it, loss=6.651404, best=6.648749, lr=4.99e-04]
Epoch 100/2000 - Loss: 6.651404, Best: 6.648749, LR: 4.99e-04
Training:   5%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                    | 107/2000 [07:42<2:16:04,  4.31s/it, loss=6.662694, best=6.648749, lr=4.99e-04]
ğŸ’¾ Best model saved! Loss: 6.645412
Training:   6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                   | 126/2000 [09:04<2:14:43,  4.31s/it, loss=6.645861, best=6.645412, lr=4.98e-04]
ğŸ’¾ Best model saved! Loss: 6.643950
Training:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                 | 168/2000 [12:05<2:11:28,  4.31s/it, loss=6.664028, best=6.643950, lr=4.96e-04]
ğŸ’¾ Best model saved! Loss: 6.641922
Training:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                 | 175/2000 [12:36<2:11:26,  4.32s/it, loss=6.661932, best=6.641922, lr=4.95e-04]
ğŸ’¾ Best model saved! Loss: 6.638936
Training:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                | 199/2000 [14:24<2:10:07,  4.34s/it, loss=6.654198, best=6.638936, lr=4.93e-04]
Epoch 200/2000 - Loss: 6.654198, Best: 6.638936, LR: 4.93e-04
Training:  13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                             | 255/2000 [18:20<2:05:19,  4.31s/it, loss=6.645435, best=6.638936, lr=4.87e-04]
ğŸ’¾ Best model saved! Loss: 6.637999
Training:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                           | 299/2000 [21:34<2:02:03,  4.31s/it, loss=6.647630, best=6.637999, lr=4.81e-04]
Epoch 300/2000 - Loss: 6.647630, Best: 6.637999, LR: 4.81e-04
Training:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                           | 317/2000 [22:47<2:00:42,  4.30s/it, loss=6.649298, best=6.637999, lr=4.79e-04]
ğŸ’¾ Best model saved! Loss: 6.637237
Training:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                          | 330/2000 [23:43<1:59:33,  4.30s/it, loss=6.649123, best=6.637237, lr=4.76e-04]
ğŸ’¾ Best model saved! Loss: 6.634734
Training:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                        | 382/2000 [27:27<1:55:57,  4.30s/it, loss=6.649038, best=6.634734, lr=4.67e-04]
ğŸ’¾ Best model saved! Loss: 6.634363
Training:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 394/2000 [28:19<1:55:13,  4.30s/it, loss=6.650773, best=6.634363, lr=4.65e-04]
ğŸ’¾ Best model saved! Loss: 6.629349
Training:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                       | 399/2000 [28:45<1:55:13,  4.32s/it, loss=6.650066, best=6.629349, lr=4.64e-04]
Epoch 400/2000 - Loss: 6.650066, Best: 6.629349, LR: 4.64e-04
Training:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                   | 478/2000 [34:20<1:49:10,  4.30s/it, loss=6.644551, best=6.629349, lr=4.46e-04]
ğŸ’¾ Best model saved! Loss: 6.629003
Training:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                  | 499/2000 [35:55<1:47:48,  4.31s/it, loss=6.647230, best=6.629003, lr=4.41e-04]
Epoch 500/2000 - Loss: 6.647230, Best: 6.629003, LR: 4.41e-04
Training:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                              | 591/2000 [42:30<1:41:16,  4.31s/it, loss=6.642761, best=6.629003, lr=4.16e-04]
ğŸ’¾ Best model saved! Loss: 6.625730
Training:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                              | 599/2000 [43:08<1:39:58,  4.28s/it, loss=6.655885, best=6.625730, lr=4.13e-04]
Epoch 600/2000 - Loss: 6.655885, Best: 6.625730, LR: 4.13e-04
Training:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                          | 699/2000 [50:18<1:33:26,  4.31s/it, loss=6.639262, best=6.625730, lr=3.82e-04]
Epoch 700/2000 - Loss: 6.639262, Best: 6.625730, LR: 3.82e-04
Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                     | 799/2000 [57:28<1:26:30,  4.32s/it, loss=6.639958, best=6.625730, lr=3.47e-04]
Epoch 800/2000 - Loss: 6.639958, Best: 6.625730, LR: 3.47e-04
Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                  | 834/2000 [1:00:11<1:36:18,  4.96s/it, loss=6.636367, best=6.625730, lr=3.35e-04]
ğŸ’¾ Best model saved! Loss: 6.621238
Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                | 899/2000 [1:05:31<1:30:03,  4.91s/it, loss=6.648846, best=6.621238, lr=3.10e-04]
Epoch 900/2000 - Loss: 6.648846, Best: 6.621238, LR: 3.10e-04
Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 988/2000 [1:12:45<1:24:38,  5.02s/it, loss=6.638624, best=6.621238, lr=2.77e-04]
ğŸ’¾ Best model saved! Loss: 6.619956
Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 999/2000 [1:13:45<1:22:52,  4.97s/it, loss=6.646456, best=6.619956, lr=2.72e-04]
Epoch 1000/2000 - Loss: 6.646456, Best: 6.619956, LR: 2.72e-04
Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 1023/2000 [1:15:38<1:20:11,  4.93s/it, loss=6.629480, best=6.619956, lr=2.63e-04]
ğŸ’¾ Best model saved! Loss: 6.617828
Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 1099/2000 [1:21:38<1:09:23,  4.62s/it, loss=6.639067, best=6.617828, lr=2.33e-04]
Epoch 1100/2000 - Loss: 6.639067, Best: 6.617828, LR: 2.33e-04
Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 1199/2000 [1:29:05<58:12,  4.36s/it, loss=6.634614, best=6.617828, lr=1.95e-04]
Epoch 1200/2000 - Loss: 6.634614, Best: 6.617828, LR: 1.95e-04
Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 1219/2000 [1:30:29<56:57,  4.38s/it, loss=6.635920, best=6.617828, lr=1.88e-04]
ğŸ’¾ Best model saved! Loss: 6.614837
Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 | 1228/2000 [1:31:09<1:00:37,  4.71s/it, loss=6.638646, best=6.614837, lr=1.84e-04]
ğŸ’¾ Best model saved! Loss: 6.613796
Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 1299/2000 [1:36:47<54:55,  4.70s/it, loss=6.634629, best=6.613796, lr=1.58e-04]
Epoch 1300/2000 - Loss: 6.634629, Best: 6.613796, LR: 1.58e-04
Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 1360/2000 [1:41:33<50:57,  4.78s/it, loss=6.632567, best=6.613796, lr=1.37e-04]
ğŸ’¾ Best model saved! Loss: 6.611748
Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 1399/2000 [1:44:32<44:31,  4.44s/it, loss=6.636470, best=6.611748, lr=1.23e-04]
Epoch 1400/2000 - Loss: 6.636470, Best: 6.611748, LR: 1.23e-04
Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 1499/2000 [1:52:06<36:09,  4.33s/it, loss=6.636359, best=6.611748, lr=9.18e-05]
Epoch 1500/2000 - Loss: 6.636359, Best: 6.611748, LR: 9.18e-05
Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 1599/2000 [1:59:20<28:57,  4.33s/it, loss=6.631925, best=6.611748, lr=6.43e-05]
Epoch 1600/2000 - Loss: 6.631925, Best: 6.611748, LR: 6.43e-05
Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 1602/2000 [1:59:29<28:55,  4.36s/it, loss=6.621115, best=6.611748, lr=6.38e-05]
ğŸ’¾ Best model saved! Loss: 6.610153
Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 1618/2000 [2:00:38<27:42,  4.35s/it, loss=6.635766, best=6.610153, lr=5.98e-05]
ğŸ’¾ Best model saved! Loss: 6.608246
Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 1699/2000 [2:06:36<21:50,  4.35s/it, loss=6.627280, best=6.608246, lr=4.15e-05]
Epoch 1700/2000 - Loss: 6.627280, Best: 6.608246, LR: 4.15e-05
Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 1799/2000 [2:13:51<14:39,  4.38s/it, loss=6.628145, best=6.608246, lr=2.38e-05]
Epoch 1800/2000 - Loss: 6.628145, Best: 6.608246, LR: 2.38e-05
Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1899/2000 [2:21:07<07:12,  4.28s/it, loss=6.625599, best=6.608246, lr=1.18e-05]
Epoch 1900/2000 - Loss: 6.625599, Best: 6.608246, LR: 1.18e-05
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1999/2000 [2:28:24<00:04,  4.86s/it, loss=6.617650, best=6.608246, lr=5.76e-06]
Epoch 2000/2000 - Loss: 6.617650, Best: 6.608246, LR: 5.76e-06
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [2:28:24<00:00,  4.45s/it, loss=6.617650, best=6.608246, lr=5.76e-06]

ğŸ’¾ Final model saved to: outputs/linear_arc_20251016_101536/checkpoints/final_model.pt
ğŸ’¾ Best model saved to: outputs/linear_arc_20251016_101536/checkpoints/best_model.pt
   Best loss: 6.608246

âœ“ Training completed! Final loss: 6.617650

ğŸ”„ Loading best model for backward SDE...
/home/barbara/Downloads/LARGE_DEVIATIONS_DM/Notebooks/my_tries/score_sde_pytorch/__personal_implementation__/ScoreNet.py:325: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(best_model_path, map_location=self.device)
âœ“ Best model loaded from outputs/linear_arc_20251016_101536/checkpoints/best_model.pt
  Epoch: 1618, Loss: 6.608246
ğŸ’¾ Loss plot saved to: outputs/linear_arc_20251016_101536/plots/training_loss.png
âœ“ Model saved to models/linear_arc_trained.pt

âœ“ Model saved to: models/linear_arc_trained.pt

âœ“ Neural network score function ready!

â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†
  BACKWARD SDE (t: T â†’ 0)
â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†

Running backward SDE...
  â€¢ Starting from 5000 points at t=19.74
âœ“ Backward completed!
  â€¢ Snapshots: 2001
  â€¢ Final time: 0.000 s

======================================================================
  RECONSTRUCTION RESULTS
======================================================================
  â€¢ Mean reconstruction error: nan
  â€¢ Initial spread: 3.0000
  â€¢ Final spread: nan
ğŸ’¾ Metrics saved to: outputs/linear_arc_20251016_101536/metrics.txt
ğŸ’¾ Comparison plot saved to: outputs/linear_arc_20251016_101536/plots/forward_backward_comparison.png

======================================================================
âœ“ Simulation completed!
ğŸ“ All outputs saved to: outputs/linear_arc_20251016_101536
======================================================================

